{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d16d58a5",
      "metadata": {
        "id": "d16d58a5"
      },
      "source": [
        "# LV1 – Obrada teksta i Part-of-Speech (POS) označavanje\n",
        "### Laboratorijska vježba 1\n",
        "**Tema:** Osnove obrade prirodnog jezika pomoću biblioteka spaCy i NLTK\n",
        "\n",
        "Ovaj notebook sadrži teorijski uvod, osnovne korake obrade teksta te zadatke za samostalni rad. Studenti mogu birati žele li koristiti *spaCy* ili *NLTK* biblioteku pri rješavanju zadataka."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "728e5e35",
      "metadata": {
        "id": "728e5e35"
      },
      "source": [
        "## Ciljevi vježbe\n",
        "- Upoznati osnovne korake obrade prirodnog jezika (NLP).\n",
        "- Primijeniti biblioteke **spaCy** i **NLTK** na obradu teksta.\n",
        "- Razumjeti i implementirati procese tokenizacije, uklanjanja zaustavnih riječi, lematizacije i POS označavanja.\n",
        "- Razviti sposobnost analize i interpretacije rezultata obrade teksta."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81c3ba0d",
      "metadata": {
        "id": "81c3ba0d"
      },
      "source": [
        "## 1. Instalacija potrebnih biblioteka"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy nltk matplotlib pandas\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIIjwtAUhRqm",
        "outputId": "ab05d916-460a-46f8-edc8-a3e530e93063"
      },
      "id": "DIIjwtAUhRqm",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.8)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.8)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39059489",
      "metadata": {
        "id": "39059489"
      },
      "source": [
        "## 2. Tokenizacija\n",
        "**Opis:** Tokenizacija je proces razdvajanja teksta na manje jedinice – tokene (riječi, interpunkcijske znakove itd.).\n",
        "\n",
        "U nastavku su prikazana dva načina tokenizacije: pomoću *spaCy* i pomoću *NLTK*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a48c5bc1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a48c5bc1",
        "outputId": "0742dffb-cede-497d-ca1c-4406d50df9db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural\n",
            "Language\n",
            "Processing\n",
            "enables\n",
            "computers\n",
            "to\n",
            "understand\n",
            "human\n",
            "language\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "text = 'Natural Language Processing enables computers to understand human language.'\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(token.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0fee9276",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fee9276",
        "outputId": "ea38e7ed-003a-45fe-c6a6-e8a9eca0600e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'enables', 'computers', 'to', 'understand', 'human', 'language', '.']\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "text = 'Natural Language Processing enables computers to understand human language.'\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95c3a757",
      "metadata": {
        "id": "95c3a757"
      },
      "source": [
        "### Zadatak 1\n",
        "Upiši vlastiti tekst i izvrši tokenizaciju pomoću obje biblioteke."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2dd9a883",
      "metadata": {
        "id": "2dd9a883"
      },
      "outputs": [],
      "source": [
        "text_1 = \"This is my own text for tokenization. It consists of several sentences and punctuation marks.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "785df350",
        "outputId": "4bbdd7dd-b897-4c37-fc56-3d200eaab2f8"
      },
      "source": [
        "tokens_nltk = word_tokenize(text_1)\n",
        "print(tokens_nltk)"
      ],
      "id": "785df350",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'my', 'own', 'text', 'for', 'tokenization', '.', 'It', 'consists', 'of', 'several', 'sentences', 'and', 'punctuation', 'marks', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_1 = nlp(text_1)\n",
        "for token in doc_1:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LW11N2JrH_4S",
        "outputId": "318f246c-0abc-496c-bde2-1c7e6a758a42"
      },
      "id": "LW11N2JrH_4S",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This\n",
            "is\n",
            "my\n",
            "own\n",
            "text\n",
            "for\n",
            "tokenization\n",
            ".\n",
            "It\n",
            "consists\n",
            "of\n",
            "several\n",
            "sentences\n",
            "and\n",
            "punctuation\n",
            "marks\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "051fae1c",
      "metadata": {
        "id": "051fae1c"
      },
      "source": [
        "## 3. Uklanjanje zaustavnih riječi (Stopwords)\n",
        "Zaustavne riječi su česte riječi koje ne doprinose značenju teksta (npr. the, is, in...)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "bf13e4d3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf13e4d3",
        "outputId": "edcc9a0e-5c3b-4afa-f59f-a93e4b5f1c49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'enables', 'computers', 'understand', 'human', 'language', '.']\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(text)\n",
        "filtered_spacy = [token.text for token in doc if not token.is_stop]\n",
        "print(filtered_spacy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "bcb3dc46",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcb3dc46",
        "outputId": "7e9eb40f-3347-45e6-b7b4-a15834aab43d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'enables', 'computers', 'understand', 'human', 'language', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_nltk = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(filtered_nltk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5538038",
      "metadata": {
        "id": "a5538038"
      },
      "source": [
        "### Zadatak 2\n",
        "Ukloni zaustavne riječi iz vlastitog teksta pomoću obje biblioteke."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "37d3c0f3",
      "metadata": {
        "id": "37d3c0f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "361c8db6-8d59-4d40-ea9e-a25d94e892c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['text', 'tokenization', '.', 'consists', 'sentences', 'punctuation', 'marks', '.']\n"
          ]
        }
      ],
      "source": [
        "doc_1 = nlp(text_1)\n",
        "filtered_spacy_1 = [token.text for token in doc_1 if not token.is_stop]\n",
        "print(filtered_spacy_1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_nltk_1 = [word for word in tokens_nltk if word.lower() not in stop_words]\n",
        "print(filtered_nltk_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig0z2L8QI6uX",
        "outputId": "bc47ba15-60ae-49aa-ba28-7642ffe11eb5"
      },
      "id": "ig0z2L8QI6uX",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['text', 'tokenization', '.', 'consists', 'several', 'sentences', 'punctuation', 'marks', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae512ad4",
      "metadata": {
        "id": "ae512ad4"
      },
      "source": [
        "## 4. Lematizacija\n",
        "Lematizacija svodi riječi na osnovni oblik (lemu)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "41ceba60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41ceba60",
        "outputId": "f2907956-9703-4532-cee4-94c35fa8dac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural         → Natural\n",
            "Language        → Language\n",
            "Processing      → processing\n",
            "enables         → enable\n",
            "computers       → computer\n",
            "to              → to\n",
            "understand      → understand\n",
            "human           → human\n",
            "language        → language\n",
            ".               → .\n"
          ]
        }
      ],
      "source": [
        "#Primjer: Lemmatizacija sa spaCy\n",
        "for token in doc:\n",
        "    print(f'{token.text:15} → {token.lemma_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "797452a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "797452a0",
        "outputId": "83197bac-31e9-4b1e-fcf9-0cef0f64ed9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'enable', 'computer', 'to', 'understand', 'human', 'language', '.']\n"
          ]
        }
      ],
      "source": [
        "#Primjer: Lemmatizacija s NLTK\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "pos_tags = pos_tag(tokens)\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'): return wordnet.ADJ\n",
        "    elif tag.startswith('V'): return wordnet.VERB\n",
        "    elif tag.startswith('N'): return wordnet.NOUN\n",
        "    elif tag.startswith('R'): return wordnet.ADV\n",
        "    else: return wordnet.NOUN\n",
        "lemmas = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
        "print(lemmas)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44f47cbd",
      "metadata": {
        "id": "44f47cbd"
      },
      "source": [
        "### Zadatak 3\n",
        "Primijeni lematizaciju na vlastiti tekst i usporedi rezultate između spaCy i NLTK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a357e693",
      "metadata": {
        "id": "a357e693",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2996e1e-cfd4-4d52-ab1a-1516c00ed19f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This            → this\n",
            "is              → be\n",
            "my              → my\n",
            "own             → own\n",
            "text            → text\n",
            "for             → for\n",
            "tokenization    → tokenization\n",
            ".               → .\n",
            "It              → it\n",
            "consists        → consist\n",
            "of              → of\n",
            "several         → several\n",
            "sentences       → sentence\n",
            "and             → and\n",
            "punctuation     → punctuation\n",
            "marks           → mark\n",
            ".               → .\n"
          ]
        }
      ],
      "source": [
        "for token in doc_1:\n",
        "    print(f'{token.text:15} → {token.lemma_}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "pos_tags_1 = pos_tag(tokens_nltk)\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'): return wordnet.ADJ\n",
        "    elif tag.startswith('V'): return wordnet.VERB\n",
        "    elif tag.startswith('N'): return wordnet.NOUN\n",
        "    elif tag.startswith('R'): return wordnet.ADV\n",
        "    else: return wordnet.NOUN\n",
        "lemmas = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags_1]\n",
        "print(lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OceTJkvJb60",
        "outputId": "764511dd-c47a-4680-ba4b-f5112217bdab"
      },
      "id": "9OceTJkvJb60",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'be', 'my', 'own', 'text', 'for', 'tokenization', '.', 'It', 'consist', 'of', 'several', 'sentence', 'and', 'punctuation', 'mark', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rezultati su isti po svemu, osim sto nltk ne mijenja riječi u lowercase."
      ],
      "metadata": {
        "id": "S2MKUCP6KFxd"
      },
      "id": "S2MKUCP6KFxd"
    },
    {
      "cell_type": "markdown",
      "id": "e1f1b920",
      "metadata": {
        "id": "e1f1b920"
      },
      "source": [
        "## 5. POS (Part-of-Speech) označavanje\n",
        "POS označavanje dodjeljuje gramatičku ulogu svakoj riječi (imenica, glagol, pridjev, prilog...)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "b25fb76c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b25fb76c",
        "outputId": "d14a7700-55ce-4183-a09f-6672939bb75c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural         → PROPN  (NNP)\n",
            "Language        → PROPN  (NNP)\n",
            "Processing      → NOUN   (NN)\n",
            "enables         → VERB   (VBZ)\n",
            "computers       → NOUN   (NNS)\n",
            "to              → PART   (TO)\n",
            "understand      → VERB   (VB)\n",
            "human           → ADJ    (JJ)\n",
            "language        → NOUN   (NN)\n",
            ".               → PUNCT  (.)\n"
          ]
        }
      ],
      "source": [
        "for token in doc:\n",
        "    print(f'{token.text:15} → {token.pos_:6} ({token.tag_})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "fe46861f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe46861f",
        "outputId": "90734dd3-9eb9-4ca5-b193-c2f9a2e571db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural         → JJ\n",
            "Language        → NNP\n",
            "Processing      → NNP\n",
            "enables         → VBZ\n",
            "computers       → NNS\n",
            "to              → TO\n",
            "understand      → VB\n",
            "human           → JJ\n",
            "language        → NN\n",
            ".               → .\n"
          ]
        }
      ],
      "source": [
        "pos_tags = pos_tag(tokens)\n",
        "for word, tag in pos_tags:\n",
        "    print(f'{word:15} → {tag}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3de5688",
      "metadata": {
        "id": "a3de5688"
      },
      "source": [
        "### Zadatak 4\n",
        "Izdvoji sve imenice i glagole iz svog teksta pomoću jedne od biblioteka."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "d93a5e36",
      "metadata": {
        "id": "d93a5e36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c63872a8-9e67-48ad-995d-ea80bb5fa8e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This            → PRON   (DT)\n",
            "is              → AUX    (VBZ)\n",
            "my              → PRON   (PRP$)\n",
            "own             → ADJ    (JJ)\n",
            "text            → NOUN   (NN)\n",
            "for             → ADP    (IN)\n",
            "tokenization    → NOUN   (NN)\n",
            ".               → PUNCT  (.)\n",
            "It              → PRON   (PRP)\n",
            "consists        → VERB   (VBZ)\n",
            "of              → ADP    (IN)\n",
            "several         → ADJ    (JJ)\n",
            "sentences       → NOUN   (NNS)\n",
            "and             → CCONJ  (CC)\n",
            "punctuation     → NOUN   (NN)\n",
            "marks           → NOUN   (NNS)\n",
            ".               → PUNCT  (.)\n"
          ]
        }
      ],
      "source": [
        "for token in doc_1:\n",
        "    print(f'{token.text:15} → {token.pos_:6} ({token.tag_})')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b5de9a2",
      "metadata": {
        "id": "6b5de9a2"
      },
      "source": [
        "## 6. Zadaci"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47b84243",
      "metadata": {
        "id": "47b84243"
      },
      "source": [
        "## Zadatak 1: Usporedi dva teksta po učestalosti riječi\n",
        "\n",
        "**Opis:**  \n",
        "Analiziraj dva različita teksta (npr. jedan o sportu, drugi o tehnologiji).  \n",
        "Nakon što provedeš tokenizaciju, uklanjanje zaustavnih riječi i lematizaciju, potrebno je:  \n",
        "- pronaći 5 najčešćih imenica u svakom tekstu,  \n",
        "- usporediti liste dobivenih imenica,  \n",
        "- zaključiti o čemu se govori u svakom tekstu.\n",
        "\n",
        "**Cilj:**  \n",
        "Razumjeti kako se analiza frekvencije riječi može koristiti za prepoznavanje teme teksta.\n",
        "\n",
        "**Upute:**  \n",
        "1. Učitaj dva različita teksta (mogu biti dvije rečenice, dva odlomka ili datoteke).  \n",
        "2. Obradi svaki tekst (tokenizacija → čišćenje → lematizacija → POS tagging).  \n",
        "3. Izdvoji samo riječi označene kao NOUN (imenice).  \n",
        "4. Prebroji pojavljivanja i prikaži 5 najčešćih.  \n",
        "5. Zaključi koja je tema svakog teksta."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_sport = \"\"\"\n",
        "The team played an intense match last night, delivering one of their strongest performances this season.\n",
        "Throughout the match, the team demonstrated exceptional teamwork, discipline, and determination.\n",
        "The coach repeatedly emphasized how important teamwork was for maintaining control during the most difficult moments of the match.\n",
        "Several players mentioned that the team had trained specifically to improve their teamwork and communication, which clearly paid off.\n",
        "Fans celebrated loudly after the match, recognizing that the team’s victory was crucial for improving their position in the championship.\n",
        "During the press conference, the coach praised the players for their strategy, dedication, and ability to adapt as the match progressed.\n",
        "He highlighted that every victory strengthens the team’s confidence and prepares them for future challenges.\n",
        "The upcoming match is even more important, as the team is competing for a spot in the finals.\n",
        "Analysts agree that if the team continues to show this level of teamwork and discipline, they have a strong chance of winning the entire championship.\n",
        "In the end, the team proved that success is not just about individual talent but about unity, effort, and the shared goal of winning the championship.\n",
        "\"\"\"\n",
        "\n",
        "text_tech = \"\"\"\n",
        "Modern technology is evolving rapidly, shaping the way people work, communicate, and solve complex problems.\n",
        "New devices and software are developed every year, pushing the boundaries of what modern technology can achieve.\n",
        "Researchers are focusing heavily on artificial intelligence, automation, and advanced data processing to create smarter and more powerful systems.\n",
        "These innovations enable companies to build faster devices, more secure software, and highly efficient solutions for everyday use.\n",
        "Experts believe that artificial intelligence will continue to transform technology by improving decision-making, optimizing workflows, and predicting user needs.\n",
        "Many companies are investing in automation technologies to reduce costs, increase productivity, and eliminate repetitive tasks.\n",
        "At the same time, advancements in data processing make it possible to analyze enormous datasets and identify patterns that were previously impossible to detect.\n",
        "This combination of artificial intelligence, automation, and data processing is driving a new era of modern technology.\n",
        "If current trends continue, technology will become even more integrated into daily life, offering smarter devices, adaptive software, and personalized solutions.\n",
        "Researchers conclude that the future of modern technology depends on continuous innovation, reliable data processing, and the responsible development of artificial intelligence.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "SQXDyoqIBhln"
      },
      "id": "SQXDyoqIBhln",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_top_nouns(text, top_n=5):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    nouns = [\n",
        "        token.lemma_.lower()\n",
        "        for token in doc\n",
        "        if token.pos_ == \"NOUN\" and not token.is_stop and token.is_alpha\n",
        "    ]\n",
        "\n",
        "    counter = Counter(nouns)\n",
        "    return counter.most_common(top_n)\n",
        "\n",
        "sport_top = extract_top_nouns(text_sport)\n",
        "tech_top = extract_top_nouns(text_tech)\n",
        "\n",
        "print(\"Top 5 NOUNs (Sport text):\")\n",
        "for word, freq in sport_top:\n",
        "    print(f\"{word}: {freq}\")\n",
        "\n",
        "print(\"Top 5 NOUNs (Tech text):\")\n",
        "for word, freq in tech_top:\n",
        "    print(f\"{word}: {freq}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuH36nGjKkHj",
        "outputId": "a58f8039-73c4-4916-98af-89e78db27903"
      },
      "id": "TuH36nGjKkHj",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 NOUNs (Sport text):\n",
            "team: 8\n",
            "match: 6\n",
            "teamwork: 4\n",
            "championship: 3\n",
            "discipline: 2\n",
            "Top 5 NOUNs (Tech text):\n",
            "technology: 7\n",
            "intelligence: 4\n",
            "processing: 4\n",
            "device: 3\n",
            "software: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tema prvog teksta je sport te timski rad i disciplina potrebna za pobjedu.\n",
        "Tema drugog teksta je tehnologija umjetne inteligencije i procesiranja podataka."
      ],
      "metadata": {
        "id": "srqwbDaFUElK"
      },
      "id": "srqwbDaFUElK"
    },
    {
      "cell_type": "markdown",
      "id": "8c585b56",
      "metadata": {
        "id": "8c585b56"
      },
      "source": [
        "## Zadatak 2: Analiza tonova (pozitivno vs. negativno)\n",
        "\n",
        "**Opis:**  \n",
        "Zadatak je provesti osnovnu analizu sentimenta.  \n",
        "Potrebno je obraditi nekoliko kratkih recenzija (npr. o filmovima, proizvodima, restoranima) i odrediti jesu li one pozitivne ili negativne.\n",
        "\n",
        "**Cilj:**  \n",
        "Pokazati kako se osnovni NLP alati mogu koristiti za analizu osjećaja u tekstu.\n",
        "\n",
        "**Upute:**  \n",
        "1. Pripremi popise riječi:  \n",
        "   - pozitivne: `[\"good\", \"great\", \"excellent\", \"amazing\", \"nice\", \"wonderful\"]`  \n",
        "   - negativne: `[\"bad\", \"poor\", \"terrible\", \"boring\", \"awful\", \"disappointing\"]`  \n",
        "2. Za svaku recenziju:  \n",
        "   - očisti tekst (ukloni stopwords, lematiziraj),  \n",
        "   - prebroji koliko pozitivnih i negativnih riječi sadrži.  \n",
        "3. Na temelju rezultata zaključi ton svake recenzije.  \n",
        "4. (Opcionalno) Prikaži rezultate u tablici ili grafu."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_words = [\"good\", \"great\", \"excellent\", \"amazing\", \"nice\", \"wonderful\"]\n",
        "negative_words = [\"bad\", \"poor\", \"terrible\", \"boring\", \"awful\", \"disappointing\"]\n",
        "\n",
        "reviews = [\n",
        "    \"The movie was amazing, the actors did a great job and the story was excellent.\",\n",
        "    \"This product is terrible, the quality is poor and it broke after one day.\",\n",
        "    \"The restaurant had good food but the service was disappointing.\",\n",
        "    \"The game was boring and the graphics were awful.\",\n",
        "    \"I had a wonderful experience, everything was nice and well prepared.\"\n",
        "]\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    lemmas = [\n",
        "        token.lemma_.lower()\n",
        "        for token in doc\n",
        "        if token.is_alpha and not token.is_stop\n",
        "    ]\n",
        "\n",
        "    pos_count = sum(1 for lemma in lemmas if lemma in positive_words)\n",
        "    neg_count = sum(1 for lemma in lemmas if lemma in negative_words)\n",
        "\n",
        "    if pos_count > neg_count:\n",
        "        sentiment = \"positive\"\n",
        "    elif neg_count > pos_count:\n",
        "        sentiment = \"negative\"\n",
        "    else:\n",
        "        sentiment = \"neutral\"\n",
        "\n",
        "    return pos_count, neg_count, sentiment\n",
        "\n",
        "print(\"Sentiment analysis results:\")\n",
        "\n",
        "for i, review in enumerate(reviews, 1):\n",
        "    pos, neg, sentiment = analyze_sentiment(review)\n",
        "    print(f\"Review {i}:\")\n",
        "    print(f\"  Text: {review}\")\n",
        "    print(f\"  Positive words: {pos}\")\n",
        "    print(f\"  Negative words: {neg}\")\n",
        "    print(f\"  Sentiment: {sentiment}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JJCd-wOMer7",
        "outputId": "ca119a32-39f9-40a1-d528-6ce069b5a77e"
      },
      "id": "3JJCd-wOMer7",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis results:\n",
            "Review 1:\n",
            "  Text: The movie was amazing, the actors did a great job and the story was excellent.\n",
            "  Positive words: 3\n",
            "  Negative words: 0\n",
            "  Sentiment: positive\n",
            "\n",
            "Review 2:\n",
            "  Text: This product is terrible, the quality is poor and it broke after one day.\n",
            "  Positive words: 0\n",
            "  Negative words: 2\n",
            "  Sentiment: negative\n",
            "\n",
            "Review 3:\n",
            "  Text: The restaurant had good food but the service was disappointing.\n",
            "  Positive words: 1\n",
            "  Negative words: 1\n",
            "  Sentiment: neutral\n",
            "\n",
            "Review 4:\n",
            "  Text: The game was boring and the graphics were awful.\n",
            "  Positive words: 0\n",
            "  Negative words: 2\n",
            "  Sentiment: negative\n",
            "\n",
            "Review 5:\n",
            "  Text: I had a wonderful experience, everything was nice and well prepared.\n",
            "  Positive words: 2\n",
            "  Negative words: 0\n",
            "  Sentiment: positive\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29571374",
      "metadata": {
        "id": "29571374"
      },
      "source": [
        "## Zadatak 3: Uredi nered/pronađi lažne riječi\n",
        "\n",
        "**Opis:**  \n",
        "Zadan je tekst koji sadrži izmišljene riječi ili “šum”.  \n",
        "Zadatak je pronaći riječi koje nisu prepoznate u jezičnom modelu (engl. *out of vocabulary words*).\n",
        "\n",
        "**Cilj:**  \n",
        "Razumjeti kako model prepoznaje poznate i nepoznate riječi te kako to može pomoći u detekciji pogrešaka u tekstu.\n",
        "\n",
        "**Upute:**  \n",
        "1. Unesi tekst koji sadrži besmislene riječi (npr. „The data blorp is analyzed using great accuracy flom.“).  \n",
        "2. Tokeniziraj tekst pomoću spaCy modela.  \n",
        "3. Provjeri svaku riječ pomoću `token.is_oov`, ako vrati `True`, riječ nije prepoznata.  \n",
        "4. Ispiši popis “nepoznatih” riječi.  \n",
        "5. (Opcionalno) Očisti tekst uklanjanjem tih riječi."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24cce210",
      "metadata": {
        "id": "24cce210"
      },
      "source": [
        "**Tekst:**\n",
        "\n",
        "> In the future, artificel intellgence will revolutionize the way we interract with technolodgy.  \n",
        "> Peaple might use smart assistents not only for work but also for personal healtcare and educattion.  \n",
        "> Yet, as systems become more compicated, ensuring data privasy and securrity will be crucial.  \n",
        "> The recent blonix project already shows how mashine learning can adapt to dynamic enviroments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "70699761",
      "metadata": {
        "id": "70699761"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "In the future, artificel intellgence will revolutionize the way we interract with technolodgy.\n",
        "Peaple might use smart assistents not only for work but also for personal healtcare and educattion.\n",
        "Yet, as systems become more compicated, ensuring data privasy and securrity will be crucial.\n",
        "The recent blonix project already shows how mashine learning can adapt to dynamic enviroments.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzM4NVawNjJz",
        "outputId": "43672d35-4eb2-4d66-8c30-bfba28179c75"
      },
      "id": "YzM4NVawNjJz",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "oov_words = sorted({token.text for token in doc if token.is_oov and token.is_alpha})\n",
        "\n",
        "print(\"OOV words found in the text:\")\n",
        "for w in oov_words:\n",
        "    print(w)\n",
        "\n",
        "cleaned_text = \" \".join([token.text for token in doc if not token.is_oov])\n",
        "\n",
        "print(\"\\nCleaned text (without OOV words):\")\n",
        "print(cleaned_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vtms-au6M2_D",
        "outputId": "b61e30d6-4bce-4588-9579-cf45d9cd0b5d"
      },
      "id": "Vtms-au6M2_D",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOV words found in the text:\n",
            "Peaple\n",
            "artificel\n",
            "assistents\n",
            "blonix\n",
            "educattion\n",
            "privasy\n",
            "securrity\n",
            "technolodgy\n",
            "\n",
            "Cleaned text (without OOV words):\n",
            "In the future , intellgence will revolutionize the way we interract with . might use smart not only for work but also for personal healtcare and . Yet , as systems become more compicated , ensuring data and will be crucial . The recent project already shows how mashine learning can adapt to dynamic enviroments .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6079cbe4",
      "metadata": {
        "id": "6079cbe4"
      },
      "source": [
        "## Zadatak 4: Tko govori o čemu?\n",
        "\n",
        "**Opis:**  \n",
        "Imate tri različita teksta iz različitih domena (npr. politika, sport, znanost).  \n",
        "Nakon obrade potrebno je prepoznati kojoj temi pojedini tekst pripada, koristeći najčešće riječi.\n",
        "\n",
        "**Cilj:**  \n",
        "Povezati statističku analizu riječi s prepoznavanjem teme teksta —> osnova za automatsku klasifikaciju dokumenata.\n",
        "\n",
        "**Upute:**  \n",
        "1. Pripremi tri teksta različitih tema.  \n",
        "2. Obradi svaki tekst kroz cijeli NLP postupak.  \n",
        "3. Izvuci 5 najčešćih imenica i glagola.  \n",
        "4. Na temelju tih riječi pokušaj zaključiti o čemu tekst govori.  \n",
        "5. (Opcionalno) Napravi jednostavan graf koji prikazuje razlike među tekstovima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "72eb1ec9",
      "metadata": {
        "id": "72eb1ec9"
      },
      "outputs": [],
      "source": [
        "text1 = \"\"\"\n",
        "The government has introduced a series of new reforms designed to improve economic stability and strengthen national policy.\n",
        "According to officials, the government believes these reforms will help reduce inflation and increase trust in public institutions.\n",
        "During a press conference, government representatives explained that the policy focuses on long-term economic growth, responsible budgeting, and transparent decision-making.\n",
        "Opposition leaders criticized the government, arguing that the reforms do not address the root causes of inflation and may place additional pressure on the middle-class population.\n",
        "Despite the criticism, the prime minister emphasized that the government must take decisive action to protect the economy.\n",
        "He stated that the policy is essential for maintaining stability, supporting national programs, and ensuring that citizens benefit from a more resilient economic system.\n",
        "The government also announced consultations with economic experts to refine the policy and monitor inflation trends.\n",
        "Overall, the government insists that the reforms represent a necessary step toward financial responsibility and sustainable development.\n",
        "\"\"\"\n",
        "\n",
        "text2 = \"\"\"\n",
        "The team delivered an outstanding performance last night, playing one of the most intense matches of the season.\n",
        "Throughout the match, the team showed great determination, teamwork, and discipline.\n",
        "The coach praised the team for maintaining focus and adapting their strategy as the match progressed.\n",
        "Fans celebrated loudly, recognizing that the team’s victory was crucial for securing their position in the championship rankings.\n",
        "During the post-match interview, the coach highlighted how preparation and teamwork were essential for winning such a competitive match.\n",
        "Several players said that the team felt more united than ever, and that their teamwork was the key factor in overcoming the toughest opponents.\n",
        "The next match will be even more important, as the team aims to qualify for the finals.\n",
        "If the team continues to play with this level of teamwork and discipline, they have a strong chance of winning the entire championship.\n",
        "\"\"\"\n",
        "\n",
        "text3 = \"\"\"\n",
        "Researchers at the university have developed a new material that significantly improves energy storage efficiency.\n",
        "The material was tested under various laboratory conditions, and researchers observed that the material maintained its structure even when exposed to high temperatures.\n",
        "According to the study, the material could transform the future of renewable energy by enabling more stable and long-lasting storage systems.\n",
        "Scientists believe that energy demand will continue to rise, making the development of advanced material technologies essential for sustainable production.\n",
        "The research team plans to publish additional data as they continue studying the material and its impact on battery performance.\n",
        "Several researchers have already suggested that this material could replace current lithium-based components used in many energy systems.\n",
        "If the material continues to show positive results, it may revolutionize energy production and create new opportunities for scientific innovation.\n",
        "Overall, the study highlights the importance of energy research and the potential of this new material to reshape modern technology.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "texts = {\n",
        "    \"Text 1\": text1,\n",
        "    \"Text 2\": text2,\n",
        "    \"Text 3\": text3\n",
        "}\n",
        "\n",
        "def extract_pos(doc, pos_tag):\n",
        "    return [\n",
        "        token.lemma_.lower()\n",
        "        for token in doc\n",
        "        if token.pos_ == pos_tag and not token.is_stop and token.is_alpha\n",
        "    ]\n",
        "\n",
        "def analyze_text(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    nouns = extract_pos(doc, \"NOUN\")\n",
        "    verbs = extract_pos(doc, \"VERB\")\n",
        "\n",
        "    top_nouns = Counter(nouns).most_common(5)\n",
        "    top_verbs = Counter(verbs).most_common(5)\n",
        "\n",
        "    return top_nouns, top_verbs\n",
        "\n",
        "\n",
        "for name, text in texts.items():\n",
        "    top_nouns, top_verbs = analyze_text(text)\n",
        "\n",
        "    print(name)\n",
        "    print(\"Top 5 nouns:\")\n",
        "    for w, f in top_nouns:\n",
        "        print(f\"  {w}: {f}\")\n",
        "\n",
        "    print(\"Top 5 verbs:\")\n",
        "    for w, f in top_verbs:\n",
        "        print(f\"  {w}: {f}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLHnLlfsOMxd",
        "outputId": "2dccc3f6-f353-47c8-be27-5c7e94410e80"
      },
      "id": "zLHnLlfsOMxd",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1\n",
            "Top 5 nouns:\n",
            "  government: 7\n",
            "  reform: 4\n",
            "  policy: 4\n",
            "  inflation: 3\n",
            "  stability: 2\n",
            "Top 5 verbs:\n",
            "  introduce: 1\n",
            "  design: 1\n",
            "  improve: 1\n",
            "  strengthen: 1\n",
            "  accord: 1\n",
            "\n",
            "Text 2\n",
            "Top 5 nouns:\n",
            "  team: 7\n",
            "  match: 5\n",
            "  teamwork: 4\n",
            "  discipline: 2\n",
            "  coach: 2\n",
            "Top 5 verbs:\n",
            "  play: 2\n",
            "  win: 2\n",
            "  deliver: 1\n",
            "  show: 1\n",
            "  praise: 1\n",
            "\n",
            "Text 3\n",
            "Top 5 nouns:\n",
            "  material: 9\n",
            "  energy: 6\n",
            "  researcher: 3\n",
            "  storage: 2\n",
            "  study: 2\n",
            "Top 5 verbs:\n",
            "  continue: 3\n",
            "  develop: 1\n",
            "  improve: 1\n",
            "  test: 1\n",
            "  observe: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prvi tekst govori o politici, drugi o sportu, a treci o znanosti."
      ],
      "metadata": {
        "id": "Dfdw_qJ9Olfr"
      },
      "id": "Dfdw_qJ9Olfr"
    },
    {
      "cell_type": "markdown",
      "id": "3a023ccd",
      "metadata": {
        "id": "3a023ccd"
      },
      "source": [
        "## Zadatak 5: Analiza političkih govora (napredni zadatak)\n",
        "\n",
        "> **Opis:**  \n",
        "> U ovom zadatku treba analizirati tekstove političkih govora i otkriti koje riječi govornici najčešće koriste kako bi naglasili svoje poruke.  \n",
        "> Cilj je otkriti koje teme i koje vrste riječi dominiraju u govoru.\n",
        "\n",
        "---\n",
        "\n",
        "**Upute:**\n",
        "1. Pronađi ili kopiraj dva kratka govora (ili odlomka) poznatih političara.  \n",
        "   Ako nemaš stvarne govore, možeš koristiti dva primjera niže.  \n",
        "2. Za svaki govor napravi kompletnu obradu teksta:\n",
        "   - tokenizacija  \n",
        "   - uklanjanje zaustavnih riječi  \n",
        "   - lematizacija  \n",
        "   - POS tagging  \n",
        "3. Izdvoji:\n",
        "   - 10 **imenica**,  \n",
        "   - 10 **glagola**,  \n",
        "   - 10 **pridjeva**.  \n",
        "4. Prikaži rezultate u **tri odvojene tablice** ili **grafovima** (koristi `pandas` i `matplotlib`).  \n",
        "5. Usporedi govore i pokušaj zaključiti:\n",
        "   - Koji govor je “pozitivniji” (više koristi riječi poput *hope*, *future*, *together*)  \n",
        "   - Koji je “defanzivniji” ili “konfliktniji” (više koristi riječi poput *fight*, *challenge*, *threat*).  \n",
        "6. Na kraju napiši **kratki zaključak (2–3 rečenice)**: kako se teme razlikuju i što dominira u svakom govoru.\n",
        "\n",
        "---\n",
        "\n",
        "**Cilj:**  \n",
        "Ovim zadatkom studenti povezuju sve što su naučili, obradu, analizu i interpretaciju teksta, u jednu cjelinu, simulirajući osnovnu NLP analizu stvarnih podataka.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "Ck_vcgjYb9IN"
      },
      "id": "Ck_vcgjYb9IN",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_A = \"\"\"My fellow citizens, today we gather not as strangers, but as a community united by our shared hopes and dreams.\n",
        "We stand at the dawn of a new era—one built on innovation, cooperation, and the unshakable belief in the potential of our people.\n",
        "The challenges before us are great, but so too is our courage and creativity.\n",
        "We will invest in education, protect our planet, and empower every individual to shape their own destiny.\n",
        "\n",
        "Let us build bridges, not walls; extend hands, not fists.\n",
        "Together, we can create a nation where opportunity is not limited to the few, but shared by all.\n",
        "Our strength lies not in fear, but in faith—in each other, in our values, and in the bright future we will create together.\n",
        "\n",
        "Let this be the generation that chooses unity over division, progress over stagnation, and hope over despair.\n",
        "Let this be the generation that dares to dream boldly, that embraces change, and that lifts one another up rather than tearing each other down.\n",
        "\n",
        "We will work to expand access to healthcare, to ensure that no family has to choose between medicine and food.\n",
        "We will support our teachers, invest in our children, and build schools that prepare every young mind for the world of tomorrow.\n",
        "We will encourage clean energy, sustainable development, and responsible stewardship of the natural resources entrusted to us.\n",
        "\n",
        "And above all, we will choose compassion—compassion for our neighbors, for our communities, and for those whose voices too often go unheard.\n",
        "Our nation is strongest when every citizen feels seen, valued, and empowered.\n",
        "\n",
        "Together, we will write a new chapter in our nation’s story—one defined not by fear or division, but by courage, unity, and purpose.\n",
        "A future filled with opportunity is within our reach, and it is a future we will build hand in hand.\n",
        "Let us move forward with confidence, with optimism, and with unwavering hope in all that we can achieve—together.\n",
        "\"\"\"\n",
        "\n",
        "text_B = \"\"\"My fellow citizens, the world we face today is uncertain and full of danger.\n",
        "Across the globe, our values are challenged, our security is tested, and our freedom is under threat.\n",
        "We cannot afford complacency or hesitation.\n",
        "We must strengthen our defenses, protect our borders, and ensure the safety of our families and our future.\n",
        "\n",
        "Our enemies seek to divide us, to weaken our resolve, and to spread fear and chaos.\n",
        "But we will not yield.\n",
        "We will act with determination, discipline, and strength.\n",
        "Every citizen has a role to play in defending our nation and preserving our way of life.\n",
        "\n",
        "We must increase our vigilance, enhance our intelligence capabilities, and give our armed forces the tools they need to counter every threat.\n",
        "We must stand firm against those who wish to undermine our democracy, whether they act from within or from beyond our borders.\n",
        "\n",
        "Let us face the challenges before us with courage, and together ensure that the next generation inherits not fear, but freedom—not weakness, but resilience.\n",
        "We will confront extremism wherever it appears.\n",
        "We will push back against hostile powers seeking to disrupt our alliances.\n",
        "We will safeguard our economy from manipulation and ensure that our industries cannot be exploited by those who do not share our values.\n",
        "\n",
        "The dangers we confront are real, and they are growing.\n",
        "Cyberattacks, disinformation campaigns, and coordinated acts of aggression threaten our stability.\n",
        "We cannot ignore these warnings; we must respond with unity and unwavering resolve.\n",
        "\n",
        "We will reinforce our border security, support law enforcement, and empower our military to defend every inch of our homeland.\n",
        "We will stand shoulder to shoulder, refusing to be intimidated, refusing to be divided, and refusing to surrender to forces that thrive on fear.\n",
        "\n",
        "Together, we will ensure that our nation remains safe, strong, and unbroken.\n",
        "We will rise to meet every threat, overcome every obstacle, and protect the sacred freedoms that define us.\n",
        "This is our duty, our responsibility, and our promise to all who come after us.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DF8ymaqucBd8"
      },
      "id": "DF8ymaqucBd8",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = {\n",
        "    \"Text 1\": text_A,\n",
        "    \"Text 2\": text_B\n",
        "}\n",
        "\n",
        "def extract_pos(doc, pos_tag):\n",
        "    return [\n",
        "        token.lemma_.lower()\n",
        "        for token in doc\n",
        "        if token.pos_ == pos_tag and not token.is_stop and token.is_alpha\n",
        "    ]\n",
        "\n",
        "def analyze_text(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    nouns = extract_pos(doc, \"NOUN\")\n",
        "    verbs = extract_pos(doc, \"VERB\")\n",
        "    adjectives = extract_pos(doc, \"ADJ\")\n",
        "\n",
        "\n",
        "    top_nouns = Counter(nouns).most_common(10)\n",
        "    top_verbs = Counter(verbs).most_common(10)\n",
        "    top_adjectives = Counter(adjectives).most_common(10)\n",
        "\n",
        "    return top_nouns, top_verbs, top_adjectives\n",
        "\n",
        "for name, text in texts.items():\n",
        "    top_nouns, top_verbs, top_adjectives = analyze_text(text)\n",
        "\n",
        "    print(name)\n",
        "    print(\"Top 10 nouns:\")\n",
        "    for w, f in top_nouns:\n",
        "        print(f\"  {w}: {f}\")\n",
        "\n",
        "    print(\"Top 10 verbs:\")\n",
        "    for w, f in top_verbs:\n",
        "        print(f\"  {w}: {f}\")\n",
        "\n",
        "    print(\"Top 10 adjectives:\")\n",
        "    for w, f in top_adjectives:\n",
        "        print(f\"  {w}: {f}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuZ3b7DOPEFr",
        "outputId": "4425c844-e921-44da-b382-587798b84b02"
      },
      "id": "DuZ3b7DOPEFr",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1\n",
            "Top 10 nouns:\n",
            "  hand: 3\n",
            "  nation: 3\n",
            "  future: 3\n",
            "  citizen: 2\n",
            "  community: 2\n",
            "  hope: 2\n",
            "  courage: 2\n",
            "  opportunity: 2\n",
            "  fear: 2\n",
            "  generation: 2\n",
            "Top 10 verbs:\n",
            "  build: 4\n",
            "  let: 4\n",
            "  choose: 3\n",
            "  share: 2\n",
            "  invest: 2\n",
            "  empower: 2\n",
            "  create: 2\n",
            "  gather: 1\n",
            "  unite: 1\n",
            "  stand: 1\n",
            "Top 10 adjectives:\n",
            "  new: 2\n",
            "  fellow: 1\n",
            "  unshakable: 1\n",
            "  great: 1\n",
            "  bright: 1\n",
            "  young: 1\n",
            "  clean: 1\n",
            "  sustainable: 1\n",
            "  responsible: 1\n",
            "  natural: 1\n",
            "\n",
            "Text 2\n",
            "Top 10 nouns:\n",
            "  freedom: 3\n",
            "  threat: 3\n",
            "  border: 3\n",
            "  citizen: 2\n",
            "  danger: 2\n",
            "  value: 2\n",
            "  security: 2\n",
            "  resolve: 2\n",
            "  fear: 2\n",
            "  nation: 2\n",
            "Top 10 verbs:\n",
            "  ensure: 4\n",
            "  refuse: 3\n",
            "  face: 2\n",
            "  protect: 2\n",
            "  seek: 2\n",
            "  divide: 2\n",
            "  act: 2\n",
            "  defend: 2\n",
            "  stand: 2\n",
            "  confront: 2\n",
            "Top 10 adjectives:\n",
            "  fellow: 1\n",
            "  uncertain: 1\n",
            "  armed: 1\n",
            "  firm: 1\n",
            "  hostile: 1\n",
            "  real: 1\n",
            "  unwavering: 1\n",
            "  safe: 1\n",
            "  strong: 1\n",
            "  unbroken: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prvi govor je pozitivniji, dok je drugi defanzivniji."
      ],
      "metadata": {
        "id": "1m4EeCpjQI9L"
      },
      "id": "1m4EeCpjQI9L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "U prvom tekstu dominiraju pozitivnije riječi, dok u drugom dominiraju riječi vezane za konflikt i nesigurnost. Prvi tekst vjerojatno poziva na izgradnju bolje budućnosti, dok drugi govori o obrani države od nekih \"prijetnji\". Drugi tekst koristi puno vise demagoških riječi i pokušava izazvati strah, dok prvi koji koristi više pozitivnih riječi nastoji ulijeti nadu i povjerenje slušatelju."
      ],
      "metadata": {
        "id": "A5yjhoRnQVwD"
      },
      "id": "A5yjhoRnQVwD"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "81c3ba0d",
        "051fae1c",
        "ae512ad4",
        "44f47cbd",
        "e1f1b920"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}